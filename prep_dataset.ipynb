{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb8ca14-ef56-4452-97a6-95247be66427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model = 't5-large' #770 million param\n",
    "#base_model = 't5-base' #220 million param\n",
    "#base_model = 't5-small' #60 million param\n",
    "dataset = 'wikisql'\n",
    "dataset_path = \"wikisql_tok_dataset\"\n",
    "training_output = \"trainoutput-wikisql\"\n",
    "batch_size = 32\n",
    "eval_size = 32\n",
    "epoch_num = 3\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import shutil\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from transformers import (AutoTokenizer,\n",
    "                          GenerationConfig,\n",
    "                          T5ForConditionalGeneration,\n",
    "                          Seq2SeqTrainer, \n",
    "                          Seq2SeqTrainingArguments, \n",
    "                          TrainerCallback,\n",
    "                          EarlyStoppingCallback,\n",
    "                          is_tensorboard_available,\n",
    "                          DataCollatorForSeq2Seq)\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(base_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "gen_cfg = GenerationConfig.from_model_config(model.config)\n",
    "gen_cfg.max_new_tokens = 128\n",
    "gen_cfg.min_length = 1\n",
    "\n",
    "train_data = load_dataset(dataset, split=\"train[:100%]\")\n",
    "eval_data = load_dataset(dataset, split=\"validation[:100%]\")\n",
    "test_data = load_dataset(dataset, split=\"test[:100%]\")\n",
    "\n",
    "def format_dataset(example):\n",
    " return {'input': 'translate to SQL: ' + example['question'], 'target': example['sql']['human_readable']}\n",
    "\n",
    "train_data = train_data.map(format_dataset, remove_columns=train_data.column_names)\n",
    "test_data = test_data.map(format_dataset, remove_columns=test_data.column_names)\n",
    "eval_data = eval_data.map(format_dataset, remove_columns=eval_data.column_names)\n",
    "\n",
    "def remove_dir(dir_path):\n",
    "    try:\n",
    "        shutil.rmtree(dir_path)\n",
    "        print(f\"Folder '{dir_path}' has been deleted.\")\n",
    "    except Exception as e:\n",
    "        # Ignore errors, you can print a message if needed\n",
    "        print(f\"Folder '{dir_path}' has been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53249be2-35a6-4c42-90f5-6e164cbb2dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_param_precision(model):\n",
    "  dtypes = {}\n",
    "  for _, p in model.named_parameters():\n",
    "      dtype = p.dtype\n",
    "      if dtype not in dtypes:\n",
    "          dtypes[dtype] = 0\n",
    "      dtypes[dtype] += p.numel()\n",
    "  total = 0\n",
    "  for k, v in dtypes.items():\n",
    "      total += v\n",
    "  for k, v in dtypes.items():\n",
    "      print(f\"{k}, {v / 10**6:.4f} M, {v / total*100:.2f} %\")\n",
    "\n",
    "def print_parameters(model):\n",
    "  # Count the total parameters\n",
    "  total_params = sum(p.numel() for p in model.parameters())\n",
    "  print(f\"Total parameters: {total_params/10**6:.4f} M\")\n",
    "\n",
    "device_map = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"{device_map} Memory Used: {model.get_memory_footprint() / 1024**2:.4f} MB\")\n",
    "print(\"\\nParameters:\")\n",
    "print_parameters(model)\n",
    "print(\"\\nData types:\")\n",
    "print_param_precision(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9631a1-5416-4c82-91c8-ebc2f3ded8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20095e22-344f-478f-8620-b5e81c8f14d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the entry with the least number of zero padding\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(base_model)\n",
    "train_data2 = load_dataset(dataset, split=\"train[:100%]\")\n",
    "\n",
    "def format_dataset(example):\n",
    "    return {'input': 'translate to SQL: ' + example['question'], 'target': example['sql']['human_readable']}\n",
    "\n",
    "tokenized_dataset = train_data2.map(format_dataset, remove_columns=train_data2.column_names)\n",
    "tokenized_dataset = tokenizer(tokenized_dataset[\"input\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Find the entry with the least number of zero padding\n",
    "min_zeros_entry = None\n",
    "min_zeros_count = float('inf')\n",
    "non_zeros_count = None\n",
    "\n",
    "for idx, input_ids in enumerate(tokenized_dataset[\"input_ids\"]):\n",
    "    # Assuming \"input_ids\" is a PyTorch tensor\n",
    "    zeros_count = torch.sum(input_ids == 0).item()  # Counting the number of zeros (padding tokens)\n",
    "    if zeros_count < min_zeros_count:\n",
    "        min_zeros_count = zeros_count\n",
    "        min_zeros_entry = idx\n",
    "        non_zeros_count = len(input_ids) - zeros_count\n",
    "\n",
    "# Print the result\n",
    "print(f\"The entry with the least number of zero padding is at index {min_zeros_entry}\")\n",
    "print(f\"Total non-padded items of the tensor index {min_zeros_entry} is {non_zeros_count}\")\n",
    "print(f\"Length of tensor at index {min_zeros_entry}: {len(input_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fcd398-7655-4d4e-9bc0-d9386ba3e7f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_features(example_batch):\n",
    "    input_encodings = tokenizer.batch_encode_plus(example_batch['input'], truncation = True, padding=\"max_length\", pad_to_max_length=True, max_length=96)\n",
    "    target_encodings = tokenizer.batch_encode_plus(example_batch['target'], truncation = True, padding=\"max_length\", pad_to_max_length=True, max_length=96)\n",
    "   \n",
    "    encodings = {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712cc95b-8780-4db3-b2a5-d3d009395b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remove_dir(dataset_path)\n",
    "train_data = train_data.map(convert_to_features, batched=True, remove_columns=train_data.column_names)\n",
    "test_data = test_data.map(convert_to_features, batched=True, remove_columns=test_data.column_names)\n",
    "eval_data = eval_data.map(convert_to_features, batched=True, remove_columns=eval_data.column_names)\n",
    "\n",
    "#columns = ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask']\n",
    "columns = ['input_ids', 'labels']\n",
    "\n",
    "train_data.set_format(type='torch', columns=columns)\n",
    "test_data.set_format(type='torch', columns=columns)\n",
    "eval_data.set_format(type='torch', columns=columns)\n",
    "\n",
    "train_data.save_to_disk(os.path.join(dataset_path,\"train\"))\n",
    "test_data.save_to_disk(os.path.join(dataset_path,\"test\"))\n",
    "eval_data.save_to_disk(os.path.join(dataset_path,\"eval\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
